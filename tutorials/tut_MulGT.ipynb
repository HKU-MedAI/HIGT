{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MulGT Usage Tutorial:\n",
    "\n",
    "**MulGT** uses the 512 $\\times$ 512 size patches segmented from the WSIs at 20x magnification as input data. In order to allow readers to better understand and practice, below we provide a complete usage tutorial.\n",
    "\n",
    "You can download the [TCGA data](https://portal.gdc.cancer.gov/) from this link. And this tutorial is divided into three main parts: Preprocessing, GPU Training, Testing, and Evaluation and is mainly built on [CLAM](https://github.com/mahmoodlab/CLAM), but we changed some codes to suit the specific needs. The details of them are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing:\n",
    "\n",
    "This section is divided into five parts: Basic Information Statistics, Patch Segmentation, Feature Extraction, Graph Generation, and Dataset Split. The details of each part are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Basic Information Statistics:\n",
    "\n",
    "Use the `generate_pl_bm` function to analyze the basic information of the WSIs. It mainly realizes the following three functions:\n",
    "\n",
    "1. **Objective Lens Magnification**: \n",
    "Count and record the target WSI file's default objective lens magnification information. The recorded data is saved in a file named `bm.csv`. The structure and content of it are shown in the table below:\n",
    "\n",
    "<center>\n",
    "\n",
    "| Column Name | Description | Data Type |\n",
    "|-------------|-------------|-----------|\n",
    "| slide_path | Saving path of WSI files | String |\n",
    "| base_mag | Default magnification of WSI files | String |\n",
    "\n",
    "</center>\n",
    "\n",
    "2. **Process List Generation**: \n",
    "Based on the target objective lens magnification and the cutting block size of the target objective lens magnification, generate the corresponding `pl_mag{target_magnification}x_patch{base_patch_size}_{target_patch_size}.csv`. The structure and content of it are shown in the table below:\n",
    "\n",
    "<center>\n",
    "\n",
    "| Column Name | Description | Data Type |\n",
    "|-------------|-------------|-----------|\n",
    "| slide_id | File name of the WSI files | String |\n",
    "\n",
    "</center>\n",
    "\n",
    "3. **Data cleaning**: \n",
    "Cleared some WSIs without default objective magnifications.\n",
    "\n",
    "##### (1) Parameter Description:\n",
    "\n",
    "- **`--WSI_dir`**: Saving directory of the WSI files.\n",
    "- **`--save_dir`**: Saving directory of the generated CSV files.\n",
    "- **`--base_patch_size`**: Patch size at the target magnification.\n",
    "- **`--target_mag`**: Target magnification.\n",
    "\n",
    "##### (2) Usage Example:\n",
    "\n",
    "In the following experiments, we use a dataset with a default 40x magnification to demonstrate, called `WSI_bm40`, and the WSI files of `WSI_bm40` are saved in the `/path/to/exp/WSI_bm40/WSI_bm40` directory. The codes for basic statistics can be structured as follows:\n",
    "\n",
    "```python\n",
    "generate_pl_bm(\n",
    "        WSI_dir=\"/path/to/exp/WSI_bm40/WSI_bm40\", \n",
    "        save_dir=\"/path/to/exp/WSI_bm40/csv\", \n",
    "        base_patch_size=512, \n",
    "        target_mag=20\n",
    ")\n",
    "```\n",
    "\n",
    "**Note**: Please replace the paths and values with those relevant to your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pl_bm(\n",
    "        WSI_dir, \n",
    "        save_dir, \n",
    "        base_patch_size, \n",
    "        target_mag\n",
    "    ):\n",
    "    import openslide, glob\n",
    "    import pandas as pd\n",
    "\n",
    "    process_list = {} \n",
    "    base_mag_csv = {\n",
    "        \"slide_path\": [],\n",
    "        \"base_mag\": []\n",
    "    }\n",
    "\n",
    "    for WSI in glob.glob(WSI_dir+\"/*\"):\n",
    "        slide = openslide.open_slide(WSI)\n",
    "        wsi_name = WSI.split(\"/\")[-1]\n",
    "        if slide.properties.get(openslide.PROPERTY_NAME_OBJECTIVE_POWER) == None:\n",
    "            continue\n",
    "\n",
    "        base_mag = int(slide.properties.get(openslide.PROPERTY_NAME_OBJECTIVE_POWER))\n",
    "        target_min_patch_size = int(base_patch_size*(base_mag/target_mag))\n",
    "\n",
    "        # Update for process_list\n",
    "        if target_min_patch_size not in process_list:\n",
    "            process_list[target_min_patch_size] = [wsi_name]\n",
    "        else:\n",
    "            process_list[target_min_patch_size].append(wsi_name)\n",
    "\n",
    "        # Update for base_mag_csv\n",
    "        base_mag_csv[\"slide_path\"].append(WSI)\n",
    "        base_mag_csv[\"base_mag\"].append(base_mag)\n",
    "\n",
    "        # save base_mag_csv.csv\n",
    "        df = pd.DataFrame(base_mag_csv)\n",
    "        df.to_csv(save_dir+f\"bm.csv\")\n",
    "\n",
    "        # save patch_size_i process_list.csv\n",
    "        for k in process_list.keys():\n",
    "            df = pd.DataFrame({\n",
    "                \"slide_id\": process_list[k]\n",
    "            })\n",
    "            df.to_csv(save_dir+f\"pl_mag{target_mag}x_patch{base_patch_size}_{k}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Patch Segmentation\n",
    "\n",
    "Patch segmentation is required to be performed based on the default magnification of the WSI files and the patch size used at target magnification, called `target_patch_size`. Finally, the x and y coordinates of the upper left corner of the patch segmentation result can be obtained. \n",
    "\n",
    "For the **MulGT**, the segmentation results at 20x magnification with a patch size of 512 are required, and this segmentation process is performed based on CLAM's `create_patches_fp.py`.\n",
    "\n",
    "##### (1) Parameter Description:\n",
    "\n",
    "- **`--source`**: Saving directory of the WSI files.\n",
    "- **`--save_dir`**: Saving directory of the patch segmentation results.\n",
    "- **`--patch_size`**: Patch size at target magnification.\n",
    "- **`--step_size`**: Step size for segmentation. If no overlap is required, this should be equal to the `patch_size`.\n",
    "- **`--seg`**: Flag to indicate whether to generate the mask.\n",
    "- **`--patch`**: Flag to indicate whether to generate the patch.\n",
    "- **`--stitch`**: Flag to indicate whether to generate the stitch.\n",
    "- **`--process_list`**: Path of the ßprocess list file, mainly utilizing the `slide_id` column, and other columns can be omitted.\n",
    "\n",
    "##### (2) Usage Example:\n",
    "The command line for patch segmentation can be structured as follows:\n",
    "\n",
    "```bash\n",
    "python create_patches_fp.py --source /path/to/exp/WSI_bm40/WSI_bm40 --save_dir /path/to/exp/WSI_bm40/segmented_patch/mag20x_patch512_1024 --patch_size 1024 --step_size 1024 --seg --patch --stitch --process_list /path/to/exp/WSI_bm40/csv/pl_mag20x_patch512_1024.csv\n",
    "```\n",
    "\n",
    "**Note**: Please replace the paths and values with those relevant to your setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Feature Extraction:\n",
    "\n",
    "Based on CLAM's `extract_features_fp.py`, some changes have been made, the feature shape extracted from each patch image can be selected, re-write the `extract_features_fp_re.py` file.\n",
    "\n",
    "##### (1) Parameter Description:\n",
    "\n",
    "- **`--csv_path`**: Path of the process list file, mainly utilizing the `slide_id` column, and other columns can be omitted.\n",
    "- **`--data_h5_dir`**: Saving directory of the patch segmentation results.\n",
    "- **`--data_slide_dir`**: Saving directory of the WSI files.\n",
    "- **`--feat_dir`**: Saving directory of the extracted features.\n",
    "- **`--batch_size`**: Batch size (e.g., `32`, `64`, etc.).\n",
    "- **`--target_patch_size`**: Size of the input image to the encoder. There is a built-in downsampling function.\n",
    "- **`--slide_ext`**: Suffix for WSI files.\n",
    "- **`--out_shape`**: Shape of the extracted feature.\n",
    "\n",
    "##### (2) Usage Example:\n",
    "\n",
    "The command line for feature extraction can be structured as follows:\n",
    "\n",
    "```bash\n",
    "python extract_features_fp.py --csv_path /path/to/WSI_bm40/csv/pl_mag20x_patch512_1024.csv --data_h5_dir /path/to/exp/WSI_bm40/segmented_patch/mag20x_patch512_1024 --data_slide_dir /path/to/exp/WSI_bm40/WSI_bm40 --feat_dir /path/to/exp/WSI_bm40/extracted_feature/mag20x_patch512_1024 --batch_size 256 --target_patch_size 1024 --slide_ext .svs --out_shape 512\n",
    "```\n",
    "\n",
    "**Note**: Please replace the paths and values with those relevant to your setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Graph Generation:\n",
    "Based on the above extracted features at 20x magnification, use the `generate_graph()` function to construct the input graph data. Each graph data includes three parts: features, coord_idx, adj_matrix, typing_label, stage_label.\n",
    "\n",
    "##### (1) Parameter Description:\n",
    "- **`--target_mag`**: Target magnification.\n",
    "- **`--bm_path`**: Path of the base magnification information file.\n",
    "- **`--feature_dir`**: Saving directory of the extracted features.\n",
    "- **`--base_patch_size`**: Patch size at the target magnification.\n",
    "- **`--save_path`**: Path of the generated graphs.\n",
    "- **`--typing_label_path`**: Path of the label file for the typing task.\n",
    "- **`--stage_label_path`**: Path of the label file for the stage task.\n",
    "\n",
    "**Note**: Please replace the paths and values with those relevant to your setup.\n",
    "##### (2) Usage Example:\n",
    "\n",
    "The code for graph generation can be structured as follows:\n",
    "\n",
    "```python\n",
    "generate_graph(\n",
    "     target_mag = 20,\n",
    "     bm_path = \"/path/to/exp/WSI_bm40/csv/bm.csv\",\n",
    "     feature_dir = \"/path/to/exp/WSI_bm40/extracted_feature/\",\n",
    "     base_patch_size = 512,\n",
    "     save_path = \"/path/to/exp/WSI_bm40/mulgt_graph/\"\n",
    "     typing_label_path = \"/path/to/exp/WSI_bm40/csv/typing_label.csv\",\n",
    "     stage_label_path = \"/path/to/exp/WSI_bm40/csv/stage_label_path.csv\"\n",
    ")\n",
    "```\n",
    "\n",
    "**Note**: Please replace the paths and values with those relevant to your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each WSI need to generate:\n",
    "# feature.pt: nodes' features, shape like: [num, feature_len]\n",
    "# c_idx.txt: coordinates corresponding to the order of nodes features above\n",
    "# adj_s.pt: linkage matrix, shape like: [num, num]\n",
    "\n",
    "def generate_graph(\n",
    "        target_mag, bm_path, feature_dir, base_patch_size, save_path, typing_label_path, stage_label_path\n",
    "    ):\n",
    "    import pandas as pd\n",
    "    import h5py, torch, os\n",
    "    import numpy as np\n",
    "\n",
    "    bm = pd.read_csv(bm_path)\n",
    "\n",
    "    stage_labels = pd.read_csv(stage_label_path)\n",
    "    stage_labels_slide_id = stage_labels['slide_id'].to_list()\n",
    "    stage_labels_label = stage_labels['label'].to_list()\n",
    "\n",
    "    stage_label_keys = list(set(stage_labels_label))\n",
    "    stage_label_values = [i for i in range(len(stage_label_keys))]\n",
    "    stage_label_dict = dict(zip(stage_label_keys, stage_label_values))\n",
    "    mapped_stage_labels_label = [stage_label_dict[i] for i in stage_labels_label]\n",
    "\n",
    "    stage_labels_dict = dict(zip(stage_labels_slide_id, mapped_stage_labels_label))\n",
    "\n",
    "    typing_labels = pd.read_csv(typing_label_path)\n",
    "    typing_labels_slide_id = typing_labels['slide_id'].to_list()\n",
    "    typing_labels_label = typing_labels['label'].to_list()\n",
    "\n",
    "    typing_label_keys = list(set(typing_labels_label))\n",
    "    typing_label_values = [i for i in range(len(typing_label_keys))]\n",
    "    typing_label_dict = dict(zip(typing_label_keys, typing_label_values))\n",
    "    mapped_typing_labels_label = [typing_label_dict[i] for i in typing_labels_label]\n",
    "\n",
    "    typing_labels_dict = dict(zip(typing_labels_slide_id, mapped_typing_labels_label))\n",
    "\n",
    "\n",
    "    def get_adj_matrix(h5_coords, target_patch_size):\n",
    "\n",
    "        adj_matrix = np.zeros((len(h5_coords), len(h5_coords)))\n",
    "        for patch_index in range(len(h5_coords)):\n",
    "            x,y = h5_coords[patch_index]\n",
    "            nb_coord_list = [\n",
    "                [x-target_patch_size, y-target_patch_size], [x, y-target_patch_size], [x+target_patch_size, y+target_patch_size],\n",
    "                [x-target_patch_size, y], [x, y], [x+target_patch_size, y],\n",
    "                [x-target_patch_size, y+target_patch_size], [x, y+target_patch_size], [x+target_patch_size, y+target_patch_size]\n",
    "            ]\n",
    "            for xy_ in nb_coord_list:\n",
    "                x_,y_ = xy_\n",
    "                if np.any(np.all(h5_coords == [x_, y_], axis=1)):\n",
    "                    to_patch_index = np.where((h5_coords==(x_,y_)).all(axis=1))[0][0]\n",
    "                    adj_matrix[patch_index, to_patch_index] = 1\n",
    "                    adj_matrix[to_patch_index, patch_index] = 1\n",
    "        return adj_matrix\n",
    "        \n",
    "    for slide_path, base_mag in zip(bm[\"slide_path\"],bm[\"base_mag\"]):\n",
    "        wsi_name = slide_path.split(\"/\")[-1].replace(\".svs\",\"\")\n",
    "        h5_name = wsi_name+\".h5\"\n",
    "        base_mag = int(base_mag)\n",
    "\n",
    "        feature_i = {}\n",
    "        all_feature = []\n",
    "        all_coord = \"\"\n",
    "\n",
    "        target_patch_size = int(base_patch_size*(base_mag/target_mag))\n",
    "        # print(feature_path,target_mag,target_patch_size,h5_name)\n",
    "        h5_path = f\"{feature_dir}/mag{target_mag}x_patch512_{target_patch_size}/h5_files/{h5_name}\"\n",
    "        h5_content = h5py.File(h5_path,'r')\n",
    "        h5_features = h5_content[\"/features\"][:]\n",
    "        h5_coords = h5_content[\"/coords\"][:]\n",
    "        \n",
    "        feature_i[target_mag] = {\n",
    "            \"features\": h5_features,\n",
    "            \"coords\": h5_coords\n",
    "        }\n",
    "\n",
    "        for coord in h5_coords:\n",
    "            x,y = coord\n",
    "            coord_i = str(x//target_patch_size) + '\\t' + str(y//target_patch_size) + '\\n'\n",
    "            all_coord+=coord_i\n",
    "\n",
    "        all_feature.append(h5_features)\n",
    "        all_feature = torch.tensor(np.concatenate(all_feature, axis=0))\n",
    "\n",
    "        adj_matrix = torch.tensor(get_adj_matrix(h5_coords, target_patch_size), dtype=torch.float64)\n",
    "\n",
    "        \n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "\n",
    "        all_coord = torch.tensor(h5_coords)\n",
    "\n",
    "        output = {\n",
    "            \"features\": all_feature,\n",
    "            \"adj_matrix\": adj_matrix,\n",
    "            \"coord_idx\": all_coord,  \n",
    "            \"stage_label\": stage_labels_dict[wsi_name],\n",
    "            \"typing_label\": typing_labels_dict[wsi_name]\n",
    "        }\n",
    "        \n",
    "        out_path = os.path.join(save_path, \"pt_files\", wsi_name)\n",
    "        torch.save(output, out_path+\".pt\")\n",
    "        \n",
    "        # torch.save(all_feature, os.path.join(out_path, \"features.pt\"))\n",
    "        # torch.save(adj_matrix, os.path.join(out_path, \"adj_s.pt\"))\n",
    "        # with open(os.path.join(out_path, \"c_idx.pt\"), \"w\") as f:\n",
    "        #     f.writelines(all_coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Dataset Split\n",
    "\n",
    "Based on CLAM's `create_split_seq.py`, some changes have been made, the input of the label file has been added, and the task has been set to any number of classifications，re-write the `create_split_seq_re.py` file. The `label.csv` needs to include three columns of useful information: case_id, slide_id and label. \n",
    "\n",
    "Because the typing and stage label have been written into the graph data, the main purpose of this step is to determine data segmentation, which has nothing to do with whether to use typing or stage label files, and can be used.\n",
    "\n",
    "##### (1) Parameter Description:\n",
    "- **`--label_path`**: Path of the label file for typing/stage classification task.\n",
    "- **`--seed`**: Random seed.\n",
    "- **`--task`**: Task name.\n",
    "- **`--k`**: Number of splits.\n",
    "- **`--label_frac`**: Fraction of labels.\n",
    "- **`--val_frac`**: Fraction of labels for validation.\n",
    "- **`--test_frac`**: Fraction of labels for test.\n",
    "- **`--save_dir`**: Saving directory for the split result.\n",
    "\n",
    "##### (2) Usage Examples:\n",
    "The command line for dataset split can be structured as follows:\n",
    "\n",
    "```bash\n",
    "python create_splits_seq_re.py --label_path /path/to/WSI_bm40/csv/typing_label.csv --seed 1 --task WSI_bm40 --label_frac 1.0 --k 10 --val_frac 0.2 --test_frac 0.2 --save_dir /path/to/WSI_bm40/\n",
    "```\n",
    "\n",
    "**Note**: Please replace the paths and values with those relevant to your setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPU training:\n",
    "\n",
    "In `main.py` and `utils/core_utils.py`, some minor additions and changes were made to enable the training of the HIGT model, and re-write the `main_re.py` and `utils/core_utils_re.py` files.\n",
    "\n",
    "##### (1) Parameter Description:\n",
    "- **`--max_epochs:`**: Number of training epochs.\n",
    "- **`--label_path:`**: Path of the label file for typing/stage file.\n",
    "- **`--drop_out`**: Enable dropout (p=0.25).\n",
    "- **`--early_stopping`**: Enable early stopping.\n",
    "- **`--lr`**: Learning rate.\n",
    "- **`--k`**: Number of folds.\n",
    "- **`--label_frac`**: Fraction of training labels.\n",
    "- **`--exp_code`**: Experiment code for saving results.\n",
    "- **`--weighted_sample`**: Enable weighted sampling.\n",
    "- **`--bag_loss`**: Loss function for slide-level classification.\n",
    "- **`--task`**: Task name.\n",
    "- **`--split_dir`**: Path of split result to use.\n",
    "- **`--model_type`**: Model type.\n",
    "- **`--log_data`**: Enable log data using tensorboard.\n",
    "- **`--data_root_dir`**: Saving directory of input data.\n",
    "- **`--results_dir:`**: Saving directory of result.\n",
    "\n",
    "MulGT Specific Parameter:\n",
    "- **`--typing_n_classes`**: Number of categories for subtype classification tasks\n",
    "- **`--stage_n_classes`**: Number of categories for stage classification tasks\n",
    "- **`--mulgt_task_type`**: Classification task type: [multi, subtype, stage]\n",
    "- **`--grad_norm`**: Enable to normalize the gradient\n",
    "- **`--mulgt_pool_method`**: Pooling method: [dense_mincut_pool, dense_diff_pool]\n",
    "\n",
    "##### (2) Usage Examples:\n",
    "\n",
    "The command line for GPU training can be structured as follows:\n",
    "\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=0 python main_re.py --max_epochs 100 --label_path /path/to/exp/WSI_bm40/csv/typing_label.csv --drop_out --early_stopping --lr 2e-4 --k 10 --label_frac 1.0 --exp_code WSI_bm40 --weighted_sample --bag_loss ce --task WSI_bm40 --split_dir /path/to/exp/WSI_bm40/splits/WSI_bm40_{label_frac*100} --model_type MulGT --log_data --data_root_dir /path/to/exp/WSI_bm40/mulgt_graph --results_dir /path/to/exp/WSI_bm40/mulgt_result --typing_n_classes 2 --stage_n_classe 2 --mulgt_task_type multi --grad_norm --mulgt_pool_method dense_diff_pool\n",
    "```\n",
    "\n",
    "**Note**: Please replace the paths and values with those relevant to your setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing and Evaluation Script:\n",
    "\n",
    "\n",
    "In `eval.py`, some minor additions and changes were made to enable the testing and evaluating of the **MulGT** model, and re-write the `eval_re.py` file.\n",
    "\n",
    "##### (1) Parameter Description:\n",
    "- **`--drop_out`**: Enable dropout (p=0.25).\n",
    "- **`--k`**: Number of folds.\n",
    "- **`--models_exp_code`**: Fraction of training labels.\n",
    "- **`--save_exp_code`**: Experiment code for saving results.\n",
    "- **`--task`**: Task name.\n",
    "- **`--model_type`**: Model type used for training.\n",
    "- **`--results_dir:`**: Results directory.\n",
    "- **`--split_dir`**: Manually specify the set of splits to use.\n",
    "- **`--data_root_dir`**: Data directory.\n",
    "\n",
    "##### (2) Usage Examples:\n",
    "\n",
    "The command line for testing and evaluation can be structured as follows:\n",
    "\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=0 python eval_re.py --drop_out --k 10 --models_exp_code WSI_bm40 --save_exp_code WSI_bm40 --task WSI_bm40 --model_type MulGT --results_dir /path/to/exp/WSI_bm40/result --split_dir /path/to/exp/WSI_bm40/splits/WSI_bm40_{label_frac*100} --data_root_dir /path/to/exp/WSI_bm40/graph\n",
    "```\n",
    "\n",
    "**Note**: Please replace the paths and values with those relevant to your setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Folder Structure Overview:\n",
    "\n",
    "The final folder structure and contents will look like this:\n",
    "\n",
    "```\n",
    "path_to/exp/\n",
    "    ├── WSI_bm40\n",
    "        ├── WSI_40\n",
    "            ├── slide_1.svs\n",
    "            ├── slide_2.svs\n",
    "            └── ...\n",
    "        ├── csv\n",
    "            ├── bm.csv\n",
    "            ├── typing/stage_label.csv\n",
    "            └── pl_mag20x_patch512_1024.csv\n",
    "        ├── segmented_patch\n",
    "            ├── mag20x_patch512_1024\n",
    "                ├── masks\n",
    "                    ├── slide_1_masks.jpg\n",
    "                    ├── slide_2_masks.jpg\n",
    "                    └── ...\n",
    "                ├── patches\n",
    "                    ├── slide_1_patches.h5\n",
    "                    ├── slide_2_patches.h5\n",
    "                    └── ...\n",
    "                ├── stitches\n",
    "                    ├── slide_1_stitches.jpg\n",
    "                    ├── slide_2_stitches.jpg\n",
    "                    └── ...\n",
    "                └── process_list_autogen.csv\n",
    "        ├── extracted_feature\n",
    "            ├── mag20x_patch512_1024\n",
    "                ├── h5_files\n",
    "                    ├── slide_1_feats.h5\n",
    "                    ├── slide_2_feats.h5\n",
    "                    └── ...\n",
    "                ├── pt_files\n",
    "                    ├── slide_1_feats.pt\n",
    "                    ├── slide_2_feats.pt\n",
    "                    └── ...      \n",
    "        ├── mulgt_graph\n",
    "            ├── mag20x_patch512_1024\n",
    "                ├── pt_files\n",
    "                    ├── slide_1_graph.pt\n",
    "                    ├── slide_2_graph.pt\n",
    "                    └── ...\n",
    "        ├── splits\n",
    "            ├── WSI_bm40_{label_frac*100}\n",
    "                ├── split_0_bool.csv\n",
    "                ├── split_0_descriptor.csv\n",
    "                ├── split_0.csv\n",
    "                └── ... \n",
    "        ├── mulgt_results\n",
    "            ├── split_0.csv\n",
    "            ├── s_0_checkpoint.pt\n",
    "            ├── split_0_results.pkl\n",
    "            └── ...\n",
    "```\n",
    "\n",
    "**Note**: Please replace the paths and values with those relevant to your setup."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
