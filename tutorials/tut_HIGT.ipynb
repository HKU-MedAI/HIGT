{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIGT Usage Tutorial:\n",
    "\n",
    "**HIGT** uses the 512 $\\times$ 512 size patches segmented from the WSIs at 5x and 10x magnification and thumbnail as the input data. In order to allow readers to better understand and practice, below we provide a complete usage tutorial.\n",
    "\n",
    "You can download the [TCGA data](https://portal.gdc.cancer.gov/) from this link. And this tutorial is divided into three main parts: Preprocessing, GPU Training, Testing, and Evaluation and is mainly built on [CLAM](https://github.com/mahmoodlab/CLAM), but we changed some codes to suit the specific needs. The details of them are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing:\n",
    "This section is mainly divided into five parts: Basic Information Statistics, Patch Segmentation, Feature Extraction, Hierarchical Graph (Tree) Generation and Dataset Split. The details of each part are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Basic Information Statistics:\n",
    "\n",
    "Use the `generate_pl_bm` function to analyze the basic information of the WSIs. It mainly realizes the following three functions:\n",
    "\n",
    "1. **Objective Lens Magnification**: \n",
    "Count and record the target WSI file's default objective lens magnification information. The recorded data is saved in a file named `bm.csv`. The structure and content of it are shown in the table below:\n",
    "\n",
    "<center>\n",
    "\n",
    "| Column Name | Description | Data Type |\n",
    "|-------------|-------------|-----------|\n",
    "| slide_path | Saving path of WSI files | String |\n",
    "| base_mag | Default magnification of WSI files | String |\n",
    "\n",
    "</center>\n",
    "\n",
    "2. **Process List Generation**: \n",
    "Based on the target objective lens magnification and the cutting block size of the target objective lens magnification, generate the corresponding `pl_mag{target_magnification}x_patch{base_patch_size}_{target_patch_size}.csv`. The structure and content of it are shown in the table below:\n",
    "\n",
    "<center>\n",
    "\n",
    "| Column Name | Description | Data Type |\n",
    "|-------------|-------------|-----------|\n",
    "| slide_id | File name of the WSI files | String |\n",
    "\n",
    "</center>\n",
    "\n",
    "3. **Data cleaning**: \n",
    "Cleared some WSIs without default objective magnifications.\n",
    "\n",
    "##### (1) Parameter Description:\n",
    "\n",
    "- **`--WSI_dir`**: Saving directory of the WSI files.\n",
    "- **`--save_dir`**: Saving directory of the generated CSV files.\n",
    "- **`--base_patch_size`**: Patch size at the target magnification.\n",
    "- **`--target_mag`**: Target magnification.\n",
    "\n",
    "##### (2) Usage Example:\n",
    "\n",
    "In the following experiments, we use a dataset with a default 40x magnification to demonstrate, called `WSI_bm40`, and the WSI files of `WSI_bm40` are saved in the `/path/to/exp/WSI_bm40/WSI_bm40` directory. The codes for basic statistics can be structured as follows:\n",
    "\n",
    "```python\n",
    "generate_pl_bm(\n",
    "        WSI_dir=\"/path/to/exp/WSI_bm40/WSI_bm40\", \n",
    "        save_dir=\"/path/to/exp/WSI_bm40/csv\", \n",
    "        base_patch_size=512, \n",
    "        target_mag=5\n",
    ")\n",
    "\n",
    "generate_pl_bm(\n",
    "        WSI_dir=\"/path/to/exp/WSI_bm40/WSI_bm40\", \n",
    "        save_dir=\"/path/to/exp/WSI_bm40/csv\", \n",
    "        base_patch_size=512, \n",
    "        target_mag=10\n",
    ")\n",
    "```\n",
    "\n",
    "**Note**: Please replace the paths and values with those relevant to your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pl_bm(\n",
    "        WSI_dir, \n",
    "        save_dir, \n",
    "        base_patch_size, \n",
    "        target_mag\n",
    "    ):\n",
    "    import openslide, glob\n",
    "    import pandas as pd\n",
    "\n",
    "    process_list = {} \n",
    "    base_mag_csv = {\n",
    "        \"slide_path\": [],\n",
    "        \"base_mag\": []\n",
    "    }\n",
    "\n",
    "    for WSI in glob.glob(WSI_dir+\"/*\"):\n",
    "        slide = openslide.open_slide(WSI)\n",
    "        wsi_name = WSI.split(\"/\")[-1]\n",
    "        if slide.properties.get(openslide.PROPERTY_NAME_OBJECTIVE_POWER) == None:\n",
    "            continue\n",
    "\n",
    "        base_mag = int(slide.properties.get(openslide.PROPERTY_NAME_OBJECTIVE_POWER))\n",
    "        target_min_patch_size = int(base_patch_size*(base_mag/target_mag))\n",
    "\n",
    "        # Update for process_list\n",
    "        if target_min_patch_size not in process_list:\n",
    "            process_list[target_min_patch_size] = [wsi_name]\n",
    "        else:\n",
    "            process_list[target_min_patch_size].append(wsi_name)\n",
    "\n",
    "        # Update for bm\n",
    "        base_mag_csv[\"slide_path\"].append(WSI)\n",
    "        base_mag_csv[\"base_mag\"].append(base_mag)\n",
    "\n",
    "        # save bm.csv\n",
    "        df = pd.DataFrame(base_mag_csv)\n",
    "        df.to_csv(save_dir+f\"bm.csv\")\n",
    "\n",
    "        # save patch_size_i process_list.csv\n",
    "        for k in process_list.keys():\n",
    "            df = pd.DataFrame({\n",
    "                \"slide_id\": process_list[k]\n",
    "            })\n",
    "            df.to_csv(save_dir+f\"pl_mag{target_mag}x_patch{base_patch_size}_{k}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Patch Segmentation:\n",
    "Patch segmentation is required to be performed based on the default magnification of the WSI files and the patch size used at target magnification, called `target_patch_size`. Finally, the x and y coordinates of the upper left corner of the patch segmentation result can be obtained. \n",
    "\n",
    "For the **HIGT**, the segmentation results at 5x and 10x magnification with a patch size of 512 are required. First the the segmentation process at 5x magnification is performed based on CLAM's `create_patches_fp.py`, and then the cutting results at 10x magnification of the corresponding data set are generated according to the cutting results at 5x magnification and the `generate_coords_file` function below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Patch segmentation at 5x magnification:\n",
    "\n",
    "##### (1) Parameter Description:\n",
    "\n",
    "- **`--source`**: Saving directory of the WSI files.\n",
    "- **`--save_dir`**: Saving directory of the patch segmentation results.\n",
    "- **`--patch_size`**: Patch size at target magnification.\n",
    "- **`--step_size`**: Step size for segmentation. If no overlap is required, this should be equal to the `patch_size`.\n",
    "- **`--seg`**: Flag to indicate whether to generate the mask.\n",
    "- **`--patch`**: Flag to indicate whether to generate the patch.\n",
    "- **`--stitch`**: Flag to indicate whether to generate the stitch.\n",
    "- **`--process_list`**: Path of the ÃŸprocess list file, mainly utilizing the `slide_id` column, and other columns can be omitted.\n",
    "\n",
    "##### (2) Usage Example:\n",
    "The command line for patch segmentation can be structured as follows:\n",
    "\n",
    "```bash\n",
    "python create_patches_fp.py --source /path/to/exp/WSI_bm40/WSI_bm40 --save_dir /path/to/exp/WSI_bm40/segmented_patch/mag5x_patch512_4096 --patch_size 4096 --step_size 4096 --seg --patch --stitch --process_list /path/to/exp/WSI_bm40/csv/pl_mag5x_patch512_4096.csv\n",
    "```\n",
    "\n",
    "**Note**: Please replace the paths and values with those relevant to your setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Generation of patch segmentation at 10x magnification:\n",
    "\n",
    "Use the `generate_coords_file` function below to generate the segmentation results at 10x magnification based on the segmentation results at 5x magnification.\n",
    "\n",
    "##### (1) Parameter Description:\n",
    "- **`--source_mag_seg_dir`**: Saving directory of the segmentation result generated at last step.\n",
    "- **`--target_mag`**: Target magnification.\n",
    "\n",
    "##### (2) Usage Example:\n",
    "\n",
    "The code for the generation of the patch segmentation can be structured as follows:\n",
    "\n",
    "```python\n",
    "generate_coords_file(\n",
    "    source_mag_seg_path=\"/path/to/exp/WSI_bm40/segmented_patch/mag5x_patch512_4096\", \n",
    "    target_mag=10\n",
    ")\n",
    "```\n",
    "\n",
    "**Note**: Please replace the paths and values with those relevant to your specific setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_coords_file(source_mag_seg_path, target_mag):\n",
    "    \n",
    "    import os, h5py, glob\n",
    "    import numpy as np\n",
    "    from wsi_core.wsi_utils import save_hdf5\n",
    "\n",
    "    cur_mag = int(source_mag_seg_path.split(\"/\")[-1].split(\"_\")[0].replace(\"mag\",\"\").replace(\"x\",\"\"))\n",
    "    cur_patch_size = int(source_mag_seg_path.split(\"/\")[-1].split(\"_\")[-1])\n",
    "    target_patch_size = int(cur_patch_size/int(target_mag/cur_mag))\n",
    "\n",
    "    for h5 in glob.glob(source_mag_seg_path+\"/patches/*\"):\n",
    "        h5_content = h5py.File(h5,'r')\n",
    "\n",
    "        coords = h5_content[\"coords\"][:]\n",
    "        save_path = \"/\".join(source_mag_seg_path.split(\"/\")[:-1])+f\"/mag{target_mag}x_patch512_{target_patch_size}/patches/\"\n",
    "\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "\n",
    "        attr = {\n",
    "            'patch_size' :            target_patch_size, \n",
    "            'patch_level' :           h5_content[\"coords\"].attrs[\"patch_level\"],\n",
    "            'downsample':             h5_content[\"coords\"].attrs[\"downsample\"],\n",
    "            'downsampled_level_dim' : h5_content[\"coords\"].attrs[\"downsampled_level_dim\"],\n",
    "            'level_dim':              h5_content[\"coords\"].attrs[\"level_dim\"],\n",
    "            'name':                   h5_content[\"coords\"].attrs[\"name\"],\n",
    "            'save_path':              save_path\n",
    "        }\n",
    "\n",
    "        h5_content.close()\n",
    "        coords_ = []\n",
    "        for coord in coords:\n",
    "            x,y = coord\n",
    "            coords_.append([x,y])\n",
    "            coords_.append([x+target_patch_size,y])\n",
    "            coords_.append([x,y+target_patch_size])\n",
    "            coords_.append([x+target_patch_size,y+target_patch_size])\n",
    "            \n",
    "        coords_ = np.array(coords_).astype(coords.dtype)\n",
    "        unique_coords = np.unique(coords_, axis=0)\n",
    "\n",
    "        save_hdf5(\n",
    "            save_path+h5.split(\"/\")[-1], \n",
    "            {\"coords\": unique_coords}, \n",
    "            {\"coords\":attr}, \n",
    "            mode=\"w\"\n",
    "        )\n",
    "\n",
    "        # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Feature Extraction:\n",
    "\n",
    "For the feature extraction at 5x and 10x magnification, based on CLAM's `extract_features_fp.py`, some changes have been made, the feature shape extracted from each patch image can be selected, re-write the `extract_features_fp_re.py` file. The feature extraction of the thumbnail of WSI can be implemented using the following `extract_features_thumb` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Feature Extraction at 5x and 10x magnification:\n",
    "\n",
    "##### (1) Parameter Description:\n",
    "\n",
    "- **`--csv_path`**: Path of the process list file, mainly utilizing the `slide_id` column, and other columns can be omitted.\n",
    "- **`--data_h5_dir`**: Saving directory of the patch segmentation results.\n",
    "- **`--data_slide_dir`**: Saving directory of the WSI files.\n",
    "- **`--feat_dir`**: Saving directory of the extracted features.\n",
    "- **`--batch_size`**: Batch size (e.g., `32`, `64`, etc.).\n",
    "- **`--target_patch_size`**: Size of the input image to the encoder. There is a built-in downsampling function.\n",
    "- **`--slide_ext`**: Suffix for WSI files.\n",
    "- **`--out_shape`**: Shape of the extracted feature.\n",
    "\n",
    "##### (2) Usage Example:\n",
    "\n",
    "The command line for feature extraction can be structured as follows:\n",
    "\n",
    "```bash\n",
    "# 5x Magnification\n",
    "python extract_features_fp_re.py --csv_path /path/to/WSI_bm40/csv/pl_mag5x_patch512_4096.csv --data_h5_dir /path/to/exp/WSI_bm40/segmented_patch/mag5x_patch512_4096 --data_slide_dir /path/to/exp/WSI_bm40/WSI_bm40 --feat_dir /path/to/exp/WSI_bm40/extracted_feature/mag5x_patch512_4096 --batch_size 256 --target_patch_size 4096 --slide_ext .svs --out_shape 1024\n",
    "\n",
    "# 10x Magnification\n",
    "python extract_features_fp_re.py --csv_path /path/to/WSI_bm40/csv/pl_mag10x_patch512_2048.csv --data_h5_dir /path/to/exp/WSI_bm40/segmented_patch/mag10x_patch512_2048 --data_slide_dir /path/to/exp/WSI_bm40/WSI_bm40 --feat_dir /path/to/exp/WSI_bm40/extracted_feature/mag10x_patch512_2048 --batch_size 256 --target_patch_size 2048 --slide_ext .svs --out_shape 1024\n",
    "```\n",
    "\n",
    "**Note**: Please replace the paths and values with those relevant to your setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Feature Extraction for Thumbnail:\n",
    "\n",
    "Based on the `extract_features_thumb` function, extract features from WSI thumbnail images.\n",
    "\n",
    "##### (1)Parameter Description:\n",
    "- **`--data_slide_dir`**: Saving directory of the WSI files.\n",
    "- **`--cuda_id`**: ID of GPUs to use.\n",
    "- **`--feat_dir`**: Saving directory of the extracted features.\n",
    "\n",
    "##### (2) Usage Example:\n",
    "\n",
    "The code for feature extraction of thumbnail can be structured as follows:\n",
    "\n",
    "```python\n",
    "feature_extract_thumb(\n",
    "    data_slide_dir=\"/path/to/exp/WSI_bm40/WSI_bm40\",\n",
    "    cuda_id=0,\n",
    "    feat_dir=\"/path/to/exp/WSI_bm40/extracted_feature\"\n",
    ")\n",
    "```\n",
    "\n",
    "**Note**: Please replace the paths and values with those relevant to your specific setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extract_thumb(\n",
    "    data_slide_dir,\n",
    "    cuda_id,\n",
    "    csv_path,\n",
    "    feat_dir\n",
    "):\n",
    "    import openslide, glob, torch, h5py, os\n",
    "    from torchvision import transforms\n",
    "    from models.resnet_custom import resnet50_baseline\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    slide_ids = df[\"slide_id\"]\n",
    "\n",
    "    device = torch.device(f\"cuda:{cuda_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = resnet50_baseline(pretrained=True).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    with torch.no_grad():\n",
    "        for slide_id in slide_ids:\n",
    "            slide_path = os.path.join(data_slide_dir,slide_id)\n",
    "            slide = openslide.OpenSlide(slide_path)\n",
    "            thumbnail_image = slide.get_thumbnail((512,512))\n",
    "            thumbnail_tensor = transform(thumbnail_image).to(device)\n",
    "\n",
    "            thumbnail_feat = model(thumbnail_tensor.unsqueeze(0))\n",
    "            out_root_path = f'{feat_dir}/thumbnail/'\n",
    "            if not os.path.exists(out_root_path):\n",
    "                os.makedirs(out_root_path)\n",
    "            with h5py.File(os.path.join(out_root_path,\"h5_files\",slide_path.split(\"/\")[-1]+\".h5\"), 'w') as h5f:\n",
    "                h5f.create_dataset('features', data=thumbnail_feat.cpu().numpy())\n",
    "                h5f.create_dataset('coords', data=[[0,0]])\n",
    "            print(slide_path.split(\"/\")[-1],\" done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Graph (Tree) Generation:\n",
    "Based on the above extracted features at 5x and 10x magnification and thumbnail, use the `generate_graph_tree()` function to construct a input graph (tree) data. \n",
    "\n",
    "##### (1) Parameter Description:\n",
    "- **`--target_mags`**: Target magnifications.\n",
    "- **`--bm_path`**: Path of the base magnification information file.\n",
    "- **`--feature_dir`**: Saving directory of the extracted features.\n",
    "- **`--base_patch_size`**: Patch size at the target magnification.\n",
    "- **`--save_path`**: Path of the generated graphs.\n",
    "\n",
    "##### (2) Usage Example:\n",
    "\n",
    "The code for graph generation can be structured as follows:\n",
    "\n",
    "```python\n",
    "generate_graph_tree(\n",
    "     target_mags = [float(\"inf\"), 5, 10],\n",
    "     bm_path = \"/path/to/exp/WSI_bm40/csv/bm.csv\",\n",
    "     feature_dir = \"/path/to/exp/WSI_bm40/extracted_feature/\",\n",
    "     base_patch_size = 512,\n",
    "     save_path = \"/path/to/exp/WSI_bm40/higt_graph/\"\n",
    ")\n",
    "```\n",
    "\n",
    "**Note**: Please replace the paths and values with those relevant to your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph_tree(\n",
    "    target_mags, bm_path, feature_dir, base_patch_size, save_path\n",
    "):\n",
    "    import os, h5py, torch\n",
    "    from torch_geometric.data import Data\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    def get_edge_index(feature_i, region_patch_size, patch_patch_size):\n",
    "    \n",
    "        start = []\n",
    "        end = []\n",
    "\n",
    "        len_thumbnail = 1\n",
    "    \n",
    "        region_mag = list(feature_i.keys())[1]\n",
    "        region_features = feature_i[region_mag][\"features\"]\n",
    "        region_coords = feature_i[region_mag][\"coords\"]\n",
    "        len_region_node = len(region_features)\n",
    "\n",
    "        patch_mag = list(feature_i.keys())[2]\n",
    "        patch_features = feature_i[patch_mag][\"features\"]\n",
    "        patch_coords = feature_i[patch_mag][\"coords\"]\n",
    "        len_patch_node = len(patch_features)\n",
    "        \n",
    "        # 1. region_level\n",
    "        for i in range(len_region_node):\n",
    "\n",
    "            # 1. thumbnail <=> region\n",
    "            region_index = i+len_thumbnail\n",
    "            x,y = region_coords[i]\n",
    "            start.append(0)\n",
    "            end.append(region_index)\n",
    "            start.append(region_index)\n",
    "            end.append(0)\n",
    "\n",
    "            # 2. region => region\n",
    "            nb_coord_list = [\n",
    "                [x-region_patch_size, y-region_patch_size], [x, y-region_patch_size], [x+region_patch_size, y+region_patch_size],\n",
    "                [x-region_patch_size, y], [x+region_patch_size, y],\n",
    "                [x-region_patch_size, y+region_patch_size], [x, y+region_patch_size], [x+region_patch_size, y+region_patch_size]\n",
    "            ]\n",
    "            for xy_ in nb_coord_list:\n",
    "                x_, y_ = xy_\n",
    "                if np.any(np.all(region_coords == [x_, y_], axis=1)):\n",
    "                    to_region_index = np.where((region_coords==(x_, y_)).all(axis=1))[0][0]+len_thumbnail\n",
    "                    start.append(region_index)\n",
    "                    end.append(to_region_index)\n",
    "                    start.append(to_region_index)\n",
    "                    end.append(region_index)\n",
    "\n",
    "        # 2. patch_level\n",
    "        for i in range(len_patch_node):\n",
    "            patch_index = i+len_thumbnail+len_region_node\n",
    "            x,y = patch_coords[i]\n",
    "\n",
    "            # 1. patch <=> region\n",
    "            region_coord_list = [[x,y]]\n",
    "            if x-patch_patch_size>0:\n",
    "                region_coord_list.append([x-patch_patch_size, y])\n",
    "            elif y-patch_patch_size>0:\n",
    "                region_coord_list.append([x, y-patch_patch_size])\n",
    "            elif x-patch_patch_size>0 and y-patch_patch_size>0:\n",
    "                region_coord_list.append([x-patch_patch_size, y-patch_patch_size])\n",
    "\n",
    "            for xy_ in region_coord_list:\n",
    "                x_,y_ = xy_\n",
    "                if np.any(np.all(region_coords == [x_, y_], axis=1)):\n",
    "                    region_index = np.where((region_coords==(x_,y_)).all(axis=1))[0][0]+len_thumbnail\n",
    "                    start.append(patch_index)\n",
    "                    end.append(region_index)\n",
    "                    start.append(region_index)\n",
    "                    end.append(patch_index)\n",
    "\n",
    "            # 2. patch <=> patch\n",
    "            nb_coord_list = [\n",
    "                [x-patch_patch_size, y-patch_patch_size], [x, y-patch_patch_size], [x+patch_patch_size, y+patch_patch_size],\n",
    "                [x-patch_patch_size, y], [x+patch_patch_size, y],\n",
    "                [x-patch_patch_size, y+patch_patch_size], [x, y+patch_patch_size], [x+patch_patch_size, y+patch_patch_size]\n",
    "            ]\n",
    "            for xy_ in nb_coord_list:\n",
    "                x_,y_ = xy_\n",
    "                if np.any(np.all(patch_coords == [x_, y_], axis=1)):\n",
    "                    to_patch_index = np.where((patch_coords==(x_,y_)).all(axis=1))[0][0]+len_thumbnail+len_region_node\n",
    "                    start.append(patch_index)\n",
    "                    end.append(to_patch_index)\n",
    "                    start.append(to_patch_index)\n",
    "                    end.append(patch_index)\n",
    "        return [start, end]\n",
    "\n",
    "    all_data = {}\n",
    "    bm = pd.read_csv(bm_path)\n",
    "    \n",
    "    for slide_path,base_mag in zip(bm['slide_path'],bm['base_mag']):\n",
    "        wsi_name = slide_path.split(\"/\")[-1].replace(\".svs\",\"\")\n",
    "        h5_name = wsi_name+\".h5\"\n",
    "        \n",
    "        feature_i = {}\n",
    "        all_feature = []\n",
    "\n",
    "        min_mag = min(target_mags)\n",
    "        min_patch_size = int(base_patch_size*(base_mag/min_mag))\n",
    "\n",
    "        for cur_mag in target_mags:\n",
    "            if cur_mag!=float(\"inf\"):\n",
    "                cur_patch_size = int(base_patch_size*(base_mag/cur_mag))\n",
    "                cur_feature_path = os.path.join(feature_dir,f\"mag{cur_mag}x_patch512_{cur_patch_size}/h5_files/{h5_name}\")\n",
    "            else:\n",
    "                cur_feature_path = os.path.join(feature_dir,f\"thumbnail/h5_files/{h5_name}\")\n",
    "            h5_content = h5py.File(cur_feature_path,'r')\n",
    "            h5_features = h5_content[\"/features\"][:]\n",
    "            h5_coords = h5_content[\"/coords\"][:]\n",
    "            if cur_mag == float(\"inf\"):\n",
    "                cur_mag = 0\n",
    "            feature_i[cur_mag] = {\n",
    "                \"features\": h5_features,\n",
    "                \"coords\": h5_coords\n",
    "            }\n",
    "            if len(h5_features.shape)<2:\n",
    "                h5_features = np.expand_dims(h5_features, 0)\n",
    "            all_feature.append(h5_features)\n",
    "\n",
    "        # generate the Data\n",
    "        # 0. preparation\n",
    "        region_mag = list(feature_i.keys())[1]\n",
    "        patch_mag = list(feature_i.keys())[2]\n",
    "\n",
    "        region_coords = feature_i[region_mag][\"coords\"]\n",
    "        patch_coords = feature_i[patch_mag][\"coords\"]\n",
    "\n",
    "\n",
    "        len_region_node, len_patch_node = len(region_coords), len(patch_coords)\n",
    "\n",
    "        region_patch_size = int((base_mag/region_mag)*base_patch_size)\n",
    "        patch_patch_size = int((base_mag/patch_mag)*base_patch_size)\n",
    "\n",
    "        # 1. x\n",
    "        all_feature = np.concatenate(all_feature, axis=0)\n",
    "\n",
    "        # 2. edge_index_tree_8nb\n",
    "        edge_index = get_edge_index(feature_i, region_patch_size, patch_patch_size)\n",
    "\n",
    "        # 3. batch\n",
    "        batch = [0]*len(all_feature)\n",
    "\n",
    "        # 4. data_id\n",
    "        data_id = wsi_name.split(\".\")[0]\n",
    "\n",
    "        # 5. node_type\n",
    "        node_type = [0]+[1]*len_region_node+[2]*len_patch_node\n",
    "\n",
    "        # 6. node_tree\n",
    "        node_tree_wo_patch = [-1]+[0]*len_region_node\n",
    "        node_tree_patch = []\n",
    "        for i in range(len_patch_node):\n",
    "            x, y = patch_coords[i]\n",
    "\n",
    "            region_coord_list = [[x,y]]\n",
    "            if x-patch_patch_size>0:\n",
    "                region_coord_list.append([x-patch_patch_size, y])\n",
    "            elif y-patch_patch_size>0:\n",
    "                region_coord_list.append([x, y-patch_patch_size])\n",
    "            elif x-patch_patch_size>0 and y-patch_patch_size>0:\n",
    "                region_coord_list.append([x-patch_patch_size, y-patch_patch_size])\n",
    "\n",
    "            for xy_ in region_coord_list:\n",
    "                x_,y_ = xy_\n",
    "                if np.any(np.all(region_coords == [x_, y_], axis=1)):\n",
    "                    region_index = np.where((region_coords==(x_,y_)).all(axis=1))[0][0]+1\n",
    "            node_tree_patch.append(region_index)\n",
    "        node_tree = node_tree_wo_patch+node_tree_patch\n",
    "\n",
    "        # 7. x_y_index \n",
    "        region_patch_size = int(base_patch_size*(base_mag/target_mags[1]))\n",
    "        patch_patch_size = int(base_patch_size*(base_mag/target_mags[2]))\n",
    "        # print(np.array([[0,0]]).shape, region_coords.shape, patch_coords/patch_patch_size)\n",
    "    \n",
    "        x_y_index = np.concatenate([\n",
    "            np.array([[0,0]]),\n",
    "            region_coords/region_patch_size,\n",
    "            patch_coords/patch_patch_size,\n",
    "        ])\n",
    "    \n",
    "        # generate the Data \n",
    "        node_attr=torch.tensor(all_feature, dtype=torch.float)\n",
    "        edge_index_tree_8nb = torch.tensor(edge_index,dtype=torch.long)\n",
    "        batch = torch.tensor(batch)\n",
    "        node_type = torch.tensor(node_type)\n",
    "        node_tree = torch.tensor(node_tree)\n",
    "        x_y_index = torch.tensor(x_y_index, dtype=torch.float)\n",
    "\n",
    "        data = Data(\n",
    "            x = node_attr,\n",
    "            edge_index_tree_8nb = edge_index_tree_8nb,\n",
    "            data_id = data_id,\n",
    "            batch = batch,\n",
    "            node_type = node_type,\n",
    "            node_tree = node_tree,\n",
    "            x_y_index = x_y_index\n",
    "        )\n",
    "        if not os.path.exists(os.path.join(save_path, \"pt_files\")):\n",
    "            os.makedirs(os.path.join(save_path, \"pt_files\"))\n",
    "\n",
    "        output_path = os.path.join(save_path, \"pt_files\", wsi_name)\n",
    "        print(wsi_name, \"done!\")\n",
    "        torch.save(data, f'{output_path}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Dataset Split\n",
    "\n",
    "Based on CLAM's `create_split_seq.py`, some changes have been made, the input of the label file has been added, and the task has been set to any number of classificationsï¼Œre-write the `create_split_seq_re.py` file. The `label.csv` needs to include three columns of useful information: case_id, slide_id and label. \n",
    "\n",
    "##### (1) Parameter Description:\n",
    "- **`--label_path`**: Path of the label file for typing/stage classification task.\n",
    "- **`--seed`**: Random seed.\n",
    "- **`--task`**: Task name.\n",
    "- **`--k`**: Number of splits.\n",
    "- **`--label_frac`**: Fraction of labels.\n",
    "- **`--val_frac`**: Fraction of labels for validation.\n",
    "- **`--test_frac`**: Fraction of labels for test.\n",
    "- **`--save_dir`**: Saving directory for the split result.\n",
    "\n",
    "##### (2) Usage Examples:\n",
    "The command line for dataset split can be structured as follows:\n",
    "\n",
    "```bash\n",
    "python create_splits_seq_re.py --label_path /path/to/WSI_bm40/csv/label.csv --seed 1 --task WSI_bm40 --label_frac 1.0 --k 10 --val_frac 0.2 --test_frac 0.2 --save_dir /path/to/WSI_bm40/\n",
    "```\n",
    "\n",
    "**Note**: Please replace the paths and values with those relevant to your setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPU Training:\n",
    "\n",
    "In `main.py` and `utils/core_utils.py`, some minor additions and changes were made to enable the training of the HIGT model, and re-write the `main_re.py` and `utils/core_utils_re.py` files.\n",
    "\n",
    "##### (1) Parameter Description:\n",
    "- **`--max_epochs:`**: Maximum number of epochs to train.\n",
    "- **`--label_path:`**: The path of the classification label.\n",
    "- **`--drop_out`**: Enable dropout (p=0.25).\n",
    "- **`--early_stopping`**: Enable early stopping.\n",
    "- **`--lr`**: Learning rate.\n",
    "- **`--k`**:Number of folds.\n",
    "- **`--label_frac`**: Fraction of training labels.\n",
    "- **`--exp_code`**: Experiment code for saving results.\n",
    "- **`--weighted_sample`**: Enable weighted sampling.\n",
    "- **`--bag_loss`**: Slide-level classification loss function.\n",
    "- **`--task`**: Task name.\n",
    "- **`--split_dir`**: Manually specify the set of splits to use.\n",
    "- **`--model_type`**: Model type used for training.\n",
    "- **`--log_data`**: Log data using tensorboard.\n",
    "- **`--data_root_dir`**: Data directory.\n",
    "- **`--results_dir:`**: Desults directory.\n",
    "\n",
    "##### (2) Usage Examples:\n",
    "\n",
    "The command line for GPU training can be structured as follows:\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=0 python main_re.py --max_epochs 100 --label_path /path/to/exp/WSI_bm40/csv/label.csv --drop_out --early_stopping --lr 2e-4 --k 10 --label_frac 1.0 --exp_code WSI_bm40 --weighted_sample --bag_loss ce --task WSI_bm40 --split_dir /path/to/exp/WSI_bm40/splits/WSI_bm40_{label_frac*100 --model_type HIGT --log_data --data_root_dir /path/to/exp/WSI_bm40/higt_graph --results_dir /path/to/exp/WSI_bm40/higt_result \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing and Evaluation:\n",
    "\n",
    "\n",
    "In `eval.py`, some minor additions and changes were made to enable the testing and evaluating of the HIGT model, and re-write the `eval_re.py` file.\n",
    "\n",
    "##### (1) Parameter Description:\n",
    "- **`--drop_out`**: Enable dropout (p=0.25).\n",
    "- **`--k`**: Number of folds.\n",
    "- **`--models_exp_code`**: Fraction of training labels.\n",
    "- **`--save_exp_code`**: Experiment code for saving results.\n",
    "- **`--task`**: Task name.\n",
    "- **`--model_type`**: Model type used for training.\n",
    "- **`--results_dir:`**: Results directory.\n",
    "- **`--split_dir`**: Manually specify the set of splits to use.\n",
    "- **`--data_root_dir`**: Data directory.\n",
    "\n",
    "##### (2) Usage Examples:\n",
    "\n",
    "The command line for testing and evaluation can be structured as follows:\n",
    "\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=0 python eval_re.py --drop_out --k 10 --models_exp_code WSI_bm40 --save_exp_code WSI_bm40 --task WSI_bm40 --model_type MulGT --results_dir /path/to/exp/WSI_bm40/higt_result --split_dir /path/to/exp/WSI_bm40/splits/WSI_bm40_{label_frac*100} --data_root_dir /path/to/exp/WSI_bm40/higt_graph\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Folder Structure Overview:\n",
    "\n",
    "The final folder structure and contents will look like this:\n",
    "\n",
    "```\n",
    "path_to/exp/\n",
    "    â”œâ”€â”€ WSI_bm40\n",
    "        â”œâ”€â”€ WSI_40\n",
    "            â”œâ”€â”€ slide_1.svs\n",
    "            â”œâ”€â”€ slide_2.svs\n",
    "            â””â”€â”€ ...\n",
    "        â”œâ”€â”€ csv\n",
    "            â”œâ”€â”€ bm.csv\n",
    "            â”œâ”€â”€ typing/stage_label.csv\n",
    "            â”œâ”€â”€ pl_mag10x_patch512_2048.csv\n",
    "            â””â”€â”€ pl_mag5x_patch512_4096.csv\n",
    "        â”œâ”€â”€ segmented_patch\n",
    "            â”œâ”€â”€ mag10x_patch512_2048\n",
    "                â”œâ”€â”€ masks\n",
    "                    â”œâ”€â”€ slide_1_masks.jpg\n",
    "                    â”œâ”€â”€ slide_2_masks.jpg\n",
    "                    â””â”€â”€ ...\n",
    "                â”œâ”€â”€ patches\n",
    "                    â”œâ”€â”€ slide_1_patches.h5\n",
    "                    â”œâ”€â”€ slide_2_patches.h5\n",
    "                    â””â”€â”€ ...\n",
    "                â”œâ”€â”€ stitches\n",
    "                    â”œâ”€â”€ slide_1_stitches.jpg\n",
    "                    â”œâ”€â”€ slide_2_stitches.jpg\n",
    "                    â””â”€â”€ ...\n",
    "                â””â”€â”€ process_list_autogen.csv\n",
    "            â”œâ”€â”€ mag5x_patch512_4096\n",
    "                â”œâ”€â”€ masks\n",
    "                    â”œâ”€â”€ slide_1_masks.jpg\n",
    "                    â”œâ”€â”€ slide_2_masks.jpg\n",
    "                    â””â”€â”€ ...\n",
    "                â”œâ”€â”€ patches\n",
    "                    â”œâ”€â”€ slide_1_patches.h5\n",
    "                    â”œâ”€â”€ slide_2_patches.h5\n",
    "                    â””â”€â”€ ...\n",
    "                â”œâ”€â”€ stitches\n",
    "                    â”œâ”€â”€ slide_1_stitches.jpg\n",
    "                    â”œâ”€â”€ slide_2_stitches.jpg\n",
    "                    â””â”€â”€ ...\n",
    "                â””â”€â”€ process_list_autogen.csv\n",
    "        â”œâ”€â”€ extracted_feature\n",
    "            â”œâ”€â”€ mag10x_patch512_2048\n",
    "                â”œâ”€â”€ h5_files\n",
    "                    â”œâ”€â”€ slide_1_feats.h5\n",
    "                    â”œâ”€â”€ slide_2_feats.h5\n",
    "                    â””â”€â”€ ...\n",
    "                â”œâ”€â”€ pt_files\n",
    "                    â”œâ”€â”€ slide_1_feats.pt\n",
    "                    â”œâ”€â”€ slide_2_feats.pt\n",
    "                    â””â”€â”€ ...      \n",
    "            â”œâ”€â”€ mag5x_patch512_4096\n",
    "                â”œâ”€â”€ h5_files\n",
    "                    â”œâ”€â”€ slide_1_feats.h5\n",
    "                    â”œâ”€â”€ slide_2_feats.h5\n",
    "                    â””â”€â”€ ...\n",
    "                â”œâ”€â”€ pt_files\n",
    "                    â”œâ”€â”€ slide_1_feats.pt\n",
    "                    â”œâ”€â”€ slide_2_feats.pt\n",
    "                    â””â”€â”€ ...      \n",
    "            â”œâ”€â”€ thumbnail\n",
    "                â”œâ”€â”€ h5_files\n",
    "                    â”œâ”€â”€ slide_1_feats.h5\n",
    "                    â”œâ”€â”€ slide_2_feats.h5\n",
    "                    â””â”€â”€ ...\n",
    "                â”œâ”€â”€ pt_files\n",
    "                    â”œâ”€â”€ slide_1_feats.pt\n",
    "                    â”œâ”€â”€ slide_2_feats.pt\n",
    "                    â””â”€â”€ ...      \n",
    "        â”œâ”€â”€ higt_graph\n",
    "            â”œâ”€â”€ pt_files\n",
    "                â”œâ”€â”€ slide_1_graph.pt\n",
    "                â”œâ”€â”€ slide_2_graph.pt\n",
    "                â””â”€â”€ ...\n",
    "        â”œâ”€â”€ splits\n",
    "            â”œâ”€â”€ WSI_bm40_{label_frac*100}\n",
    "                â”œâ”€â”€ split_0_bool.csv\n",
    "                â”œâ”€â”€ split_0_descriptor.csv\n",
    "                â”œâ”€â”€ split_0.csv\n",
    "                â””â”€â”€ ... \n",
    "        â”œâ”€â”€ higt_results\n",
    "            â”œâ”€â”€ split_0.csv\n",
    "            â”œâ”€â”€ s_0_checkpoint.pt\n",
    "            â”œâ”€â”€ split_0_results.pkl\n",
    "            â””â”€â”€ ...\n",
    "```\n",
    "\n",
    "**Note**: Please replace the paths and values with those relevant to your setup."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
